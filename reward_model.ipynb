{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, PreTrainedTokenizerFast, TrainingArguments, Trainer\n",
    "from datasets import load_dataset, load_from_disk\n",
    "from trl import RewardTrainer\n",
    "\n",
    "from pprint import pprint\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find optimal device\n",
    "def optimal_device():\n",
    "    if not torch.cuda.is_available():\n",
    "        print(\"Sorry did not detect a cuda device.\")\n",
    "        return torch.device('cpu')\n",
    "    \n",
    "    device = 0\n",
    "    memory = 0\n",
    "    for d in range(torch.cuda.device_count()):\n",
    "        mem = torch.cuda.mem_get_info(d)\n",
    "        print(f'cuda id={d} device={torch.cuda.get_device_name(0)} memory={mem[1]} free={mem[0]}')\n",
    "        if mem[0] / mem[1] > memory:\n",
    "            memory = mem[0] / mem[1]\n",
    "            device = d\n",
    "    return torch.device(f\"cuda:{device}\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for GPU availability\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"Using device: {torch.cuda.get_device_name({device})}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pretrained DeBERTa model\n",
    "model_id = \"microsoft/deberta-base\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_id, num_labels=1)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "print(f\"Using Fast Tokenizer: {tokenizer.is_fast}\")\n",
    "# fast_tokenizer = PreTrainedTokenizerFast(tokenizer_object=tokenizer)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load RLHF dataset\n",
    "dataset_id = \"Dahoas/full-hh-rlhf\"\n",
    "dataset = load_dataset(dataset_id)\n",
    "\n",
    "train_dataset = dataset['train']\n",
    "eval_dataset = dataset['test']\n",
    "\n",
    "print(train_dataset.column_names)\n",
    "print(len(train_dataset))\n",
    "print(len(eval_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    model.config.pad_token_id = model.config.eos_token_id\n",
    "    \n",
    "print(tokenizer.pad_token)\n",
    "print(model.config.pad_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatting_func(examples,):\n",
    "    kwargs = {\"padding\": \"max_length\", \"truncation\": True, \"max_length\": 512, \"return_tensors\": \"pt\"}\n",
    "\n",
    "    prompt_plus_chosen_response = examples[\"prompt\"] + \"\\n\" + examples[\"chosen\"]\n",
    "    prompt_plus_rejected_response = examples[\"prompt\"] + \"\\n\" + examples[\"chosen\"]\n",
    "    tokens_chosen = tokenizer.encode_plus(prompt_plus_chosen_response, **kwargs)\n",
    "    tokens_rejected = tokenizer.encode_plus(prompt_plus_rejected_response, **kwargs)\n",
    "\n",
    "    return {\n",
    "        \"input_ids_chosen\": tokens_chosen[\"input_ids\"][0], \"attention_mask_chosen\": tokens_chosen[\"attention_mask\"][0],\n",
    "        \"input_ids_rejected\": tokens_rejected[\"input_ids\"][0], \"attention_mask_rejected\": tokens_rejected[\"attention_mask\"][0]\n",
    "    }\n",
    "\n",
    "# Format dataset for model ingestion\n",
    "formatted_dataset = dataset.map(formatting_func)\n",
    "\n",
    "# Save the formatted dataset\n",
    "formatted_dataset.save_to_disk(\"datasets/formatted_dataset\", )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load formatted dataset\n",
    "formatted_dataset = load_from_disk(\"datasets/formatted_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(formatted_dataset.column_names)\n",
    "print(formatted_dataset[\"train\"].features)\n",
    "print(formatted_dataset[\"train\"][0])\n",
    "print(formatted_dataset[\"test\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",          # output directory for model and logs\n",
    "    evaluation_strategy=\"epoch\",     # evaluate each epoch\n",
    "    learning_rate=2e-5,              # learning rate\n",
    "    per_device_train_batch_size=16,  # batch size for training\n",
    "    per_device_eval_batch_size=64,   # batch size for evaluation\n",
    "    gradient_accumulation_steps=4,   # accumulate gradients\n",
    "    gradient_checkpointing=True,     # use gradient checkpointing\n",
    "    num_train_epochs=3,              # number of training epochs\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    save_strategy=\"epoch\",           # save model each epoch\n",
    "    load_best_model_at_end=True,     # load the best model when finished training\n",
    ")\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = RewardTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=formatted_dataset[\"train\"],\n",
    "    eval_dataset=formatted_dataset[\"test\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    model.config.pad_token_id = model.config.eos_token_id\n",
    "    \n",
    "print(tokenizer.pad_token)\n",
    "print(model.config.pad_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model.save_pretrained(\"./models/deberta-rlhf-v1\")\n",
    "\n",
    "# Push model to Huggingface Hub\n",
    "trainer.push_to_hub()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
