{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d51aa35-cbaa-4368-86f0-f767b12df300",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import transformers\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, PreTrainedTokenizerFast, TrainingArguments, Trainer, AutoModel\n",
    "from datasets import load_dataset, load_from_disk\n",
    "device = \"cuda\"\n",
    "def preprocess(example):\n",
    "    example=example[\"prompt\"]\n",
    "    return tokenizer(example, return_tensors=\"pt\", padding=True, truncation=True, max_length=256).to(device)\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, tokenizer, examples):\n",
    "        self.examples=examples\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        example=self.examples[idx][\"prompt\"]\n",
    "        return tokenizer(example, return_tensors=\"pt\", padding=True, truncation=True, max_length=256).to(device)\n",
    "\n",
    "def freeze_model(model):\n",
    "    for name, param in model.named_parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "def copy_model_parameters(old_model, new_model):\n",
    "    for param_old, param_new in zip(old_model.parameters(), new_model.parameters()):\n",
    "        param_old.data.copy_(param_new.data)\n",
    "\n",
    "def compute_prob(logits, output_length):\n",
    "    generated_logits = logits[:, -output_length:, :]\n",
    "    generated_probs = F.softmax(generated_logits, dim=-1)\n",
    "    sequence_prob = torch.prod(torch.diagonal(generated_probs[:, :-1], dim1=1, dim2=2))\n",
    "    return sequence_prob\n",
    "\n",
    "def nll(logits, output_length):\n",
    "    generated_logits = logits[:, -output_length:, :]\n",
    "    probs = F.softmax(generated_logits, dim=-1)\n",
    "\n",
    "    # Compute negative log-likelihood\n",
    "    nll = -torch.sum(torch.log(probs) * probs, dim=-1)\n",
    "\n",
    "    # Sum the negative log-likelihood for all tokens\n",
    "    total_nll = torch.sum(nll)\n",
    "    return total_nll\n",
    "    \n",
    "\"\"\"\n",
    "we maintain 2 policies, an old policy and new policy (both LMs), and then we generate whole trajectiry for input data using old_policy for a batch. We also have a inear layer on top of output embeddings which gives value function estimate for each step of the generation. Now for that batch, for lets say we wanna run 4 PPO epochs per batch, so we compute value function estimate using new policy, run grad descent on new_policy for the batch for 4 ppo epochs, then we uodate old policy with new policy, compute generation for thre batch using old_policy, and then keep on doing this.\n",
    "\"\"\"\n",
    "#TODO: Implement past_key_values to fasten up training\n",
    "\n",
    "reward_name = \"OpenAssistant/reward-model-deberta-v3-large-v2\"\n",
    "reward_model, reward_tokenizer = AutoModelForSequenceClassification.from_pretrained(reward_name).to(device), AutoTokenizer.from_pretrained(reward_name)\n",
    "freeze_model(reward_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "babbb7d8-9bc5-4fd0-b10a-f0031af2f233",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id=\"microsoft/deberta-v3-large\"\n",
    "# assuming reward_name and model_id have both same tokenier, else some weird shit can go down, number of states mismatch and so on\n",
    "# policy_tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "# old_policy=AutoModel.from_pretrained(model_id).to(device)\n",
    "\n",
    "old_policy=AutoModel.from_pretrained(model_id).to(device).base_model\n",
    "new_policy=AutoModel.from_pretrained(model_id).to(device).base_model\n",
    "config = old_policy.config\n",
    "# new_policy=AutoModel.from_pretrained(model_id).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "759c7c48-d871-4c69-9111-bc8a6f5f28ec",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Couldn't instantiate the backend tokenizer from one of: \n(1) a `tokenizers` library serialization file, \n(2) a slow tokenizer instance to convert or \n(3) an equivalent slow tokenizer class to instantiate and convert. \nYou need to have sentencepiece installed to convert a slow tokenizer to a fast one.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DebertaV2TokenizerFast\n\u001b[0;32m----> 2\u001b[0m policy_tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mDebertaV2TokenizerFast\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmicrosoft/deberta-v3-large\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_fast\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# policy_tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=False)\u001b[39;00m\n\u001b[1;32m      4\u001b[0m embedding_dim \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mhidden_size\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2089\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   2086\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2087\u001b[0m         logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloading file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m from cache at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresolved_vocab_files[file_id]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 2089\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_from_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2090\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresolved_vocab_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2091\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2092\u001b[0m \u001b[43m    \u001b[49m\u001b[43minit_configuration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2093\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minit_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2094\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2095\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2096\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2097\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2098\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_is_local\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_local\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2099\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2100\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2101\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2311\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._from_pretrained\u001b[0;34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, token, cache_dir, local_files_only, _commit_hash, _is_local, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   2309\u001b[0m \u001b[38;5;66;03m# Instantiate the tokenizer.\u001b[39;00m\n\u001b[1;32m   2310\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2311\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minit_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2312\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[1;32m   2313\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[1;32m   2314\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to load vocabulary from file. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2315\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease check that the provided vocabulary is accessible and not corrupted.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2316\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/transformers/models/deberta_v2/tokenization_deberta_v2_fast.py:103\u001b[0m, in \u001b[0;36mDebertaV2TokenizerFast.__init__\u001b[0;34m(self, vocab_file, tokenizer_file, do_lower_case, split_by_punct, bos_token, eos_token, unk_token, sep_token, pad_token, cls_token, mask_token, **kwargs)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     90\u001b[0m     vocab_file\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    102\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 103\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvocab_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtokenizer_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdo_lower_case\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdo_lower_case\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    107\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbos_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbos_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[43m        \u001b[49m\u001b[43meos_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meos_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    109\u001b[0m \u001b[43m        \u001b[49m\u001b[43munk_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munk_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[43m        \u001b[49m\u001b[43msep_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msep_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcls_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcls_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmask_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_by_punct\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msplit_by_punct\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdo_lower_case \u001b[38;5;241m=\u001b[39m do_lower_case\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msplit_by_punct \u001b[38;5;241m=\u001b[39m split_by_punct\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/transformers/tokenization_utils_fast.py:120\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m     fast_tokenizer \u001b[38;5;241m=\u001b[39m convert_slow_tokenizer(slow_tokenizer)\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 120\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    121\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt instantiate the backend tokenizer from one of: \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    122\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(1) a `tokenizers` library serialization file, \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    123\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(2) a slow tokenizer instance to convert or \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    124\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(3) an equivalent slow tokenizer class to instantiate and convert. \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    125\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou need to have sentencepiece installed to convert a slow tokenizer to a fast one.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    126\u001b[0m     )\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tokenizer \u001b[38;5;241m=\u001b[39m fast_tokenizer\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m slow_tokenizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mValueError\u001b[0m: Couldn't instantiate the backend tokenizer from one of: \n(1) a `tokenizers` library serialization file, \n(2) a slow tokenizer instance to convert or \n(3) an equivalent slow tokenizer class to instantiate and convert. \nYou need to have sentencepiece installed to convert a slow tokenizer to a fast one."
     ]
    }
   ],
   "source": [
    "from transformers import DebertaV2TokenizerFast\n",
    "policy_tokenizer = DebertaV2TokenizerFast.from_pretrained(\"microsoft/deberta-v3-large\", use_fast=False)\n",
    "# policy_tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=False)\n",
    "embedding_dim = config.hidden_size\n",
    "linear = nn.Linear(embedding_dim, 1, device=device)\n",
    "\n",
    "dataset_id = \"Dahoas/full-hh-rlhf\"\n",
    "dataset = load_dataset(dataset_id)\n",
    "\n",
    "train_dataset = CustomDataset(tokenizer, dataset['train'])\n",
    "eval_dataset = CustomDataset(tokenizer, dataset['test'])\n",
    "\n",
    "# print(train_dataset.column_names)\n",
    "print(len(train_dataset))\n",
    "print(len(eval_dataset))\n",
    "\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = rank_tokenizer.eos_token\n",
    "    reward_model.config.pad_token_id = reward_model.config.eos_token_id\n",
    "    old_policy.config.pad_token_id = policy_model.config.eos_token_id\n",
    "    new_policy.config.pad_token_id = policy_model.config.eos_token_id\n",
    "\n",
    "print(reward_model.config.pad_token_id == reward_model.config.eos_token_id)\n",
    "assert policy_tokenizer.get_config() == reward_tokenizer.get_config()\n",
    "\n",
    "batch_size = 4\n",
    "dataloader = DataLoader(dataset['train'], batch_size=batch_size, shuffle=True)\n",
    "max_length=512 # total sequence length input+output\n",
    "gamma=1.\n",
    "c1=1.\n",
    "c2=1.\n",
    "eps=0.1\n",
    "epochs=1\n",
    "# policy= model.base_model # the underlying LM? dont keep shared parameters, doesnt work, freeze reward model, make copy for policy\n",
    "ppo_iters_per_batch =4 # gotta do 4 updates per batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "362e4aff-6ccc-4ed6-9849-cea61b75c5a7",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "DebertaV2Model.forward() got an unexpected keyword argument 'max_length'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 24\u001b[0m\n\u001b[1;32m     22\u001b[0m copy_model_parameters(old_policy, new_policy)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# inputs=policy_tokenizer(batch, return_tensors=\"pt\").to(device)\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m outputs_old_policy\u001b[38;5;241m=\u001b[39m\u001b[43mold_policy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m outputs_ids_old_policy \u001b[38;5;241m=\u001b[39m outputs_old_policy\u001b[38;5;241m.\u001b[39mlogits\u001b[38;5;241m.\u001b[39margmax(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     26\u001b[0m logits_old_policy \u001b[38;5;241m=\u001b[39m outputs_old_policy\u001b[38;5;241m.\u001b[39mlogits\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: DebertaV2Model.forward() got an unexpected keyword argument 'max_length'"
     ]
    }
   ],
   "source": [
    "def entropy_from_logits(logits):\n",
    "    pd = torch.nn.functional.softmax(logits, dim=-1)\n",
    "    entropy = torch.logsumexp(logits, axis=-1) - torch.sum(pd * logits, axis=-1)\n",
    "    return entropy\n",
    "\n",
    "def masked_mean(values, mask, axis= None):\n",
    "    if axis is not None:\n",
    "        return (values * mask).sum(axis=axis) / mask.sum(axis=axis)\n",
    "    else:\n",
    "        return (values * mask).sum() / mask.sum()\n",
    "\n",
    "def logprobs_from_logits(logits, labels, gather = True):\n",
    "    logp = F.log_softmax(logits, dim=2)\n",
    "    if not gather:\n",
    "        return logp\n",
    "    logpy = torch.gather(logp, 2, labels.unsqueeze(2)).squeeze(-1)\n",
    "    return logpy\n",
    "\n",
    "\n",
    "for e in range(epochs):\n",
    "    for batch in dataloader:\n",
    "        copy_model_parameters(old_policy, new_policy)\n",
    "        # inputs=policy_tokenizer(batch, return_tensors=\"pt\").to(device)\n",
    "        outputs_old_policy=old_policy(batch, max_length=max_length)\n",
    "        outputs_ids_old_policy = outputs_old_policy.logits.argmax(-1)\n",
    "        logits_old_policy = outputs_old_policy.logits\n",
    "        print(logits_old_policy.shape)\n",
    "        old_policy_logprobs = logprobs_from_logits(logits_old_policy)\n",
    "        # values_old=linear(outputs_old_policy.hidden_state[-1])\n",
    "        episode_length = max_length - batch.input_ids.shape[1]\n",
    "        freeze_model(old_policy)\n",
    "        \n",
    "        # mini_batch_gen_output_length = episode_length .....\n",
    "        # aint doing mini-batches as its not done in the paper we use as reference (mini-batch of size 1 means takimng whole batch itself as mini-batch)\n",
    "        output_attn_mask=(outputs_ids_old_policy != old_policy.config.pad_token_id).float().to(device)\n",
    "        assert output_attn_mask.shape == old_policy_logprobs.shape, (output_attn_mask.shape, old_policy_prob.shape) \n",
    "        old_policy_output_logprob = (masked_mean(old_policy_logprobs, output_attn_mask))\n",
    "        useful_output_length = torch.max(torch.sum(output_attn_mask, dim=0,keepdim=True)).item().cpu()\n",
    "        \n",
    "        \n",
    "        concatenated_input = {\n",
    "                                'input_ids': torch.cat([batch[\"input_ids\"], outputs_ids_old_policy], dim=1),\n",
    "                                'attention_mask': torch.cat([batch[\"attention_mask\"], output_attn_mask], dim=1)\n",
    "                            }\n",
    "        for iter in range(ppo_iters_per_batch):\n",
    "            outputs_new_policy=new_policy(concatenated_input)\n",
    "            assert outputs_new_policy.shape[1] == max_length\n",
    "            values_new=linear(outputs_new_policy.hidden_state[-1][-(episode_length+1):-1])\n",
    "            # outputs_ids_old_policy = outputs_old_policy.logits.argmax(-1)\n",
    "            # logits_old_policy = outputs_old_policy.logits\n",
    "\n",
    "            # discounted_rewards=[]\n",
    "            reversed_discounted_rewards=[0.]\n",
    "            # rewards=[]\n",
    "            reversed_rewards=[]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            for t in range(useful_output_length):\n",
    "                reversed_rewards.append(reward_model(concatenated_input).logits[0].cpu())\n",
    "                reversed_discounted_rewards.append(reward_model(concatenated_input).logits[0].cpu() + gamma*reversed_discounted_rewards[-1])\n",
    "                # reversed_advantages.append(reversed_rewards[-1] + gamma*reversed_advantages[-1]\n",
    "            reversed_discounted_rewards.pop(0)\n",
    "            reversed_rewards = torch.Tensor(reversed_rewards).to(device)\n",
    "            reversed_discounted_rewards=torch.Tensor(reveersed_discounted_rewards).to(device)\n",
    "            \n",
    "            rewards=reversed_rewards.flip(0, inplace=False)\n",
    "            discounted_rewards=reversed_discounted_rewards.flip(0, inplace=False)\n",
    "            \n",
    "            advantages=-values_new + discounted_rewards\n",
    "            logits_new_policy = logprobs_from_logits(outputs_new_policy.logits[-episode_length:])\n",
    "            new_policy_logprobs = logprobs_from_logits(logits_new_policy)\n",
    "            assert output_attn_mask.shape == new_policy_logprobs.shape, (output_attn_mask.shape, new_policy_prob.shape) \n",
    "            new_policy_output_logprob = (masked_mean(new_policy_probs, output_attn_mask))\n",
    "            ratio = torch.exp(new_policy_output_prob - old_policy_output_prob)\n",
    "            print(\"ratio shape\", ratio.shape)\n",
    "            print(\"advantage shape\", advantage.shape)\n",
    "            l_clip = torch.mean(torch.mul(advantages,torch.clamp(ratio, 1-eps, 1+eps))) ## clip loss\n",
    "            loss -= c1*torch.mean(advantages**2) ## mse loss \n",
    "            loss += c2* entropy_from_logits(logits_new_policy)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
