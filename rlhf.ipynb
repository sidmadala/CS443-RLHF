{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82b86744",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sidmadala/miniconda3/envs/cs443/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import transformers\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, PreTrainedTokenizerFast, TrainingArguments, Trainer, AutoModel\n",
    "from datasets import load_dataset, load_from_disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "242ebe26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n",
      "Using device: NVIDIA A100-SXM4-80GB\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"Using device: {torch.cuda.get_device_name({device})}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d51aa35-cbaa-4368-86f0-f767b12df300",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(example):\n",
    "    example=example[\"prompt\"]\n",
    "    return tokenizer(example, return_tensors=\"pt\", padding=True, truncation=True, max_length=256).to(device)\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, tokenizer, examples):\n",
    "        self.examples=examples\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        example=self.examples[idx][\"prompt\"]\n",
    "        return tokenizer(example, return_tensors=\"pt\", padding=True, truncation=True, max_length=256).to(device)\n",
    "\n",
    "def freeze_model(model):\n",
    "    for name, param in model.named_parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "def copy_model_parameters(old_model, new_model):\n",
    "    for param_old, param_new in zip(old_model.parameters(), new_model.parameters()):\n",
    "        param_old.data.copy_(param_new.data)\n",
    "\n",
    "def compute_prob(logits, output_length):\n",
    "    generated_logits = logits[:, -output_length:, :]\n",
    "    generated_probs = F.softmax(generated_logits, dim=-1)\n",
    "    sequence_prob = torch.prod(torch.diagonal(generated_probs[:, :-1], dim1=1, dim2=2))\n",
    "    return sequence_prob\n",
    "\n",
    "def nll(logits, output_length):\n",
    "    generated_logits = logits[:, -output_length:, :]\n",
    "    probs = F.softmax(generated_logits, dim=-1)\n",
    "\n",
    "    # Compute negative log-likelihood\n",
    "    nll = -torch.sum(torch.log(probs) * probs, dim=-1)\n",
    "\n",
    "    # Sum the negative log-likelihood for all tokens\n",
    "    total_nll = torch.sum(nll)\n",
    "    return total_nll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c6b99a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "we maintain 2 policies, an old policy and new policy (both LMs), and then we generate whole trajectiry for input data using old_policy for a batch. We also have a inear layer on top of output embeddings which gives value function estimate for each step of the generation. Now for that batch, for lets say we wanna run 4 PPO epochs per batch, so we compute value function estimate using new policy, run grad descent on new_policy for the batch for 4 ppo epochs, then we uodate old policy with new policy, compute generation for thre batch using old_policy, and then keep on doing this.\n",
    "\"\"\"\n",
    "#TODO: Implement past_key_values to fasten up training\n",
    "\n",
    "reward_name = \"OpenAssistant/reward-model-deberta-v3-large-v2\"\n",
    "reward_model, reward_tokenizer = AutoModelForSequenceClassification.from_pretrained(reward_name).to(device), AutoTokenizer.from_pretrained(reward_name)\n",
    "freeze_model(reward_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "babbb7d8-9bc5-4fd0-b10a-f0031af2f233",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id=\"microsoft/deberta-v3-large\"\n",
    "# assuming reward_name and model_id have both same tokenier, else some weird shit can go down, number of states mismatch and so on\n",
    "# policy_tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "# old_policy=AutoModel.from_pretrained(model_id).to(device)\n",
    "\n",
    "old_policy=AutoModel.from_pretrained(model_id).to(device).base_model\n",
    "new_policy=AutoModel.from_pretrained(model_id).to(device).base_model\n",
    "config = old_policy.config\n",
    "# new_policy=AutoModel.from_pretrained(model_id).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "759c7c48-d871-4c69-9111-bc8a6f5f28ec",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m dataset_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDahoas/full-hh-rlhf\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      8\u001b[0m dataset \u001b[38;5;241m=\u001b[39m load_dataset(dataset_id)\n\u001b[0;32m---> 10\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m CustomDataset(\u001b[43mtokenizer\u001b[49m, dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     11\u001b[0m eval_dataset \u001b[38;5;241m=\u001b[39m CustomDataset(tokenizer, dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# print(train_dataset.column_names)\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "from transformers import DebertaV2Tokenizer\n",
    "policy_tokenizer = DebertaV2Tokenizer.from_pretrained(\"microsoft/deberta-v3-large\", use_fast=False)\n",
    "# policy_tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=False)\n",
    "embedding_dim = config.hidden_size\n",
    "linear = nn.Linear(embedding_dim, 1, device=device)\n",
    "\n",
    "dataset_id = \"Dahoas/full-hh-rlhf\"\n",
    "dataset = load_dataset(dataset_id)\n",
    "\n",
    "train_dataset = CustomDataset(tokenizer, dataset['train'])\n",
    "eval_dataset = CustomDataset(tokenizer, dataset['test'])\n",
    "\n",
    "# print(train_dataset.column_names)\n",
    "print(len(train_dataset))\n",
    "print(len(eval_dataset))\n",
    "\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = rank_tokenizer.eos_token\n",
    "    reward_model.config.pad_token_id = reward_model.config.eos_token_id\n",
    "    old_policy.config.pad_token_id = policy_model.config.eos_token_id\n",
    "    new_policy.config.pad_token_id = policy_model.config.eos_token_id\n",
    "\n",
    "print(reward_model.config.pad_token_id == reward_model.config.eos_token_id)\n",
    "assert policy_tokenizer.get_config() == reward_tokenizer.get_config()\n",
    "\n",
    "batch_size = 4\n",
    "dataloader = DataLoader(dataset['train'], batch_size=batch_size, shuffle=True)\n",
    "max_length=512 # total sequence length input+output\n",
    "gamma=1.\n",
    "c1=1.\n",
    "c2=1.\n",
    "eps=0.1\n",
    "epochs=1\n",
    "# policy= model.base_model # the underlying LM? dont keep shared parameters, doesnt work, freeze reward model, make copy for policy\n",
    "ppo_iters_per_batch =4 # gotta do 4 updates per batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362e4aff-6ccc-4ed6-9849-cea61b75c5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy_from_logits(logits):\n",
    "    pd = torch.nn.functional.softmax(logits, dim=-1)\n",
    "    entropy = torch.logsumexp(logits, axis=-1) - torch.sum(pd * logits, axis=-1)\n",
    "    return entropy\n",
    "\n",
    "def masked_mean(values, mask, axis= None):\n",
    "    if axis is not None:\n",
    "        return (values * mask).sum(axis=axis) / mask.sum(axis=axis)\n",
    "    else:\n",
    "        return (values * mask).sum() / mask.sum()\n",
    "\n",
    "def logprobs_from_logits(logits, labels, gather = True):\n",
    "    logp = F.log_softmax(logits, dim=2)\n",
    "    if not gather:\n",
    "        return logp\n",
    "    logpy = torch.gather(logp, 2, labels.unsqueeze(2)).squeeze(-1)\n",
    "    return logpy\n",
    "\n",
    "\n",
    "for e in range(epochs):\n",
    "    for batch in dataloader:\n",
    "        copy_model_parameters(old_policy, new_policy)\n",
    "        # inputs=policy_tokenizer(batch, return_tensors=\"pt\").to(device)\n",
    "        outputs_old_policy=old_policy(batch, max_length=max_length)\n",
    "        outputs_ids_old_policy = outputs_old_policy.logits.argmax(-1)\n",
    "        logits_old_policy = outputs_old_policy.logits\n",
    "        print(logits_old_policy.shape)\n",
    "        old_policy_logprobs = logprobs_from_logits(logits_old_policy)\n",
    "        # values_old=linear(outputs_old_policy.hidden_state[-1])\n",
    "        episode_length = max_length - batch.input_ids.shape[1]\n",
    "        freeze_model(old_policy)\n",
    "        \n",
    "        # mini_batch_gen_output_length = episode_length .....\n",
    "        # aint doing mini-batches as its not done in the paper we use as reference (mini-batch of size 1 means takimng whole batch itself as mini-batch)\n",
    "        output_attn_mask=(outputs_ids_old_policy != old_policy.config.pad_token_id).float().to(device)\n",
    "        assert output_attn_mask.shape == old_policy_logprobs.shape, (output_attn_mask.shape, old_policy_prob.shape) \n",
    "        old_policy_output_logprob = (masked_mean(old_policy_logprobs, output_attn_mask))\n",
    "        useful_output_length = torch.max(torch.sum(output_attn_mask, dim=0,keepdim=True)).item().cpu()\n",
    "        \n",
    "        \n",
    "        concatenated_input = {\n",
    "                                'input_ids': torch.cat([batch[\"input_ids\"], outputs_ids_old_policy], dim=1),\n",
    "                                'attention_mask': torch.cat([batch[\"attention_mask\"], output_attn_mask], dim=1)\n",
    "                            }\n",
    "        for iter in range(ppo_iters_per_batch):\n",
    "            outputs_new_policy=new_policy(concatenated_input)\n",
    "            assert outputs_new_policy.shape[1] == max_length\n",
    "            values_new=linear(outputs_new_policy.hidden_state[-1][-(episode_length+1):-1])\n",
    "            # outputs_ids_old_policy = outputs_old_policy.logits.argmax(-1)\n",
    "            # logits_old_policy = outputs_old_policy.logits\n",
    "\n",
    "            # discounted_rewards=[]\n",
    "            reversed_discounted_rewards=[0.]\n",
    "            # rewards=[]\n",
    "            reversed_rewards=[]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            for t in range(useful_output_length):\n",
    "                reversed_rewards.append(reward_model(concatenated_input).logits[0].cpu())\n",
    "                reversed_discounted_rewards.append(reward_model(concatenated_input).logits[0].cpu() + gamma*reversed_discounted_rewards[-1])\n",
    "                # reversed_advantages.append(reversed_rewards[-1] + gamma*reversed_advantages[-1]\n",
    "            reversed_discounted_rewards.pop(0)\n",
    "            reversed_rewards = torch.Tensor(reversed_rewards).to(device)\n",
    "            reversed_discounted_rewards=torch.Tensor(reveersed_discounted_rewards).to(device)\n",
    "            \n",
    "            rewards=reversed_rewards.flip(0, inplace=False)\n",
    "            discounted_rewards=reversed_discounted_rewards.flip(0, inplace=False)\n",
    "            \n",
    "            advantages=-values_new + discounted_rewards\n",
    "            logits_new_policy = logprobs_from_logits(outputs_new_policy.logits[-episode_length:])\n",
    "            new_policy_logprobs = logprobs_from_logits(logits_new_policy)\n",
    "            assert output_attn_mask.shape == new_policy_logprobs.shape, (output_attn_mask.shape, new_policy_prob.shape) \n",
    "            new_policy_output_logprob = (masked_mean(new_policy_probs, output_attn_mask))\n",
    "            ratio = torch.exp(new_policy_output_prob - old_policy_output_prob)\n",
    "            print(\"ratio shape\", ratio.shape)\n",
    "            print(\"advantage shape\", advantage.shape)\n",
    "            l_clip = torch.mean(torch.mul(advantages,torch.clamp(ratio, 1-eps, 1+eps))) ## clip loss\n",
    "            loss -= c1*torch.mean(advantages**2) ## mse loss \n",
    "            loss += c2* entropy_from_logits(logits_new_policy)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
