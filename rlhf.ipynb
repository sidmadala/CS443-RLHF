{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82b86744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n",
      "Using device: NVIDIA A100-SXM4-80GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import transformers\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, PreTrainedTokenizerFast, TrainingArguments, Trainer, AutoModel\n",
    "from datasets import load_dataset, load_from_disk\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"Using device: {torch.cuda.get_device_name({device})}\")\n",
    "\n",
    "## it seems policy LM cant ber similar to deberta as its encoder only, so need to use gpt-2 for policy, so tokenization mismatch is eminent\n",
    "\n",
    "# def preprocess(example):\n",
    "#     example=example[\"prompt\"]\n",
    "#     return tokenizer(example, return_tensors=\"pt\", padding=\"longest\", truncation=True, max_length=256).to(device)\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, tokenizer, examples):\n",
    "        self.examples=examples\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.examples[idx][\"prompt\"]\n",
    "        example=self.examples[idx][\"prompt\"]\n",
    "        out_ = self.tokenizer(example, return_tensors=\"pt\", padding=True, truncation=True, max_length=256).to(device)\n",
    "        # return out_\n",
    "        for key in out_.keys():\n",
    "            out_[key] = out_[key].squeeze(0)\n",
    "        return out_\n",
    "\n",
    "def freeze_model(model):\n",
    "    for name, param in model.named_parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "def copy_model_parameters(old_model, new_model):\n",
    "    for param_old, param_new in zip(old_model.parameters(), new_model.parameters()):\n",
    "        param_old.data.copy_(param_new.data)\n",
    "\n",
    "#TODO: reimplement ALL functions below as verbatim taken from library\n",
    "\n",
    "def compute_prob(logits, output_length):\n",
    "    generated_logits = logits[:, -output_length:, :]\n",
    "    generated_probs = F.softmax(generated_logits, dim=-1)\n",
    "    sequence_prob = torch.prod(torch.diagonal(generated_probs[:, :-1], dim1=1, dim2=2))\n",
    "    return sequence_prob\n",
    "\n",
    "def nll(logits, output_length):\n",
    "    generated_logits = logits[:, -output_length:, :]\n",
    "    probs = F.softmax(generated_logits, dim=-1)\n",
    "\n",
    "    # Compute negative log-likelihood\n",
    "    nll = -torch.sum(torch.log(probs) * probs, dim=-1)\n",
    "\n",
    "    # Sum the negative log-likelihood for all tokens\n",
    "    total_nll = torch.sum(nll)\n",
    "    return total_nll\n",
    "\n",
    "def entropy_from_logits(logits):\n",
    "    pd = torch.nn.functional.softmax(logits, dim=-1)\n",
    "    entropy = torch.logsumexp(logits, axis=-1) - torch.sum(pd * logits, axis=-1)\n",
    "    return entropy\n",
    "\n",
    "def masked_mean(values, mask, axis= None):\n",
    "    if axis is not None:\n",
    "        return (values * mask).sum(axis=axis) / mask.sum(axis=axis)\n",
    "    else:\n",
    "        return (values * mask).sum() / mask.sum()\n",
    "\n",
    "def logprobs_from_logits(logits, labels, gather = True):\n",
    "    logp = F.log_softmax(logits, dim=2)\n",
    "    if not gather:\n",
    "        return logp\n",
    "    logpy = torch.gather(logp, 2, labels.unsqueeze(2)).squeeze(-1)\n",
    "    return logpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5707e97a-e61e-4210-9c9e-73343dd2f845",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sharma96/conda_envs/rl/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "we maintain 2 policies, an old policy and new policy (both LMs), and then we generate whole trajectory for input data using old_policy for a batch.\n",
    "We also have a inear layer on top of output embeddings which gives value function estimate for each step of the generation. Now for that batch, for lets say\n",
    " we wanna run 4 PPO epochs per batch, so we compute value function estimate using new policy, run grad descent on new_policy for the batch for 4 ppo epochs, \n",
    " then we uodate old policy with new policy, compute generation for thre batch using old_policy, and then keep on doing this.\n",
    "\"\"\"\n",
    "\n",
    "#TODO: Integrate past_key_values to fasten up training\n",
    "# from transformers import LlamaTokenizer, LlamaForCausalLM\n",
    "# policy_model_path = 'openlm-research/open_llama_3b'\n",
    "# policy_tokenizer = LlamaTokenizer.from_pretrained(policy_model_path)\n",
    "# old_policy = LlamaForCausalLM.from_pretrained(policy_model_path, torch_dtype=torch.float16)\n",
    "# freeze_model(old_policy)\n",
    "\n",
    "\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, GPT2Config, GPT2TokenizerFast, AutoModelForCausalLM\n",
    "model_id=\"gpt2\"\n",
    "\n",
    "#Policy models\n",
    "old_policy=AutoModelForCausalLM.from_pretrained(model_id).to(device)\n",
    "freeze_model(old_policy)\n",
    "new_policy=AutoModelForCausalLM.from_pretrained(model_id).to(device)\n",
    "config = old_policy.config\n",
    "policy_tokenizer1 = AutoTokenizer.from_pretrained(model_id, padding=\"max_length\", direction=\"left\", padding_side=\"left\",max_length=256, length=256)\n",
    "policy_tokenizer2 = AutoTokenizer.from_pretrained(model_id, padding=\"max_length\", direction=\"left\",  padding_side=\"left\",max_length=512,length=512)\n",
    "embedding_dim = config.hidden_size\n",
    "linear = nn.Linear(embedding_dim, 1, device=device)\n",
    "\n",
    "# if policy_tokenizer.pad_token is None:\n",
    "#     policy_tokenizer.pad_token = policy_tokenizer.eos_token\n",
    "#     old_policy.config.pad_token_id = old_policy.config.eos_token_id\n",
    "#     new_policy.config.pad_token_id = new_policy.config.eos_token_id\n",
    "\n",
    "\n",
    "\n",
    "# reward_tokenizer = AutoTokenizer.from_pretrained('Ray2333/gpt2-large-harmless-reward_model')\n",
    "reward_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "                'Ray2333/gpt2-large-harmless-reward_model',\n",
    "                num_labels=1).to(device)\n",
    "freeze_model(reward_model)\n",
    "\n",
    "# if reward_tokenizer.pad_token is None:\n",
    "#     reward_tokenizer.pad_token = reward_tokenizer.eos_token\n",
    "#     reward_model.config.pad_token_id = reward_model.config.eos_token_id\n",
    "\n",
    "if policy_tokenizer1.pad_token is None:\n",
    "    policy_tokenizer1.pad_token = policy_tokenizer1.eos_token\n",
    "    policy_tokenizer2.pad_token = policy_tokenizer2.eos_token\n",
    "    old_policy.config.pad_token_id = old_policy.config.eos_token_id\n",
    "    new_policy.config.pad_token_id = new_policy.config.eos_token_id\n",
    "if reward_model.config.pad_token_id is None:\n",
    "    reward_model.config.pad_token_id = old_policy.config.pad_token_id\n",
    "    \n",
    "# print(policy_tokenizer1.pad_token_id)\n",
    "## assuming reward and policy models here are having same tokenizers atleast T_T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55a64431-ec22-44b2-8f14-58332d799170",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(old_policy.config.pad_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15decfa7-5f11-4c43-acc8-e9f91f09036e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config = GPT2Config.from_pretrained('Ray2333/gpt2-large-harmless-reward_model')\n",
    "# config2 = GPT2Config.from_pretrained(\"gpt2\")\n",
    "# print(config)\n",
    "# print(config2)\n",
    "# tokenizer = policy_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "759c7c48-d871-4c69-9111-bc8a6f5f28ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112052\n",
      "12451\n"
     ]
    }
   ],
   "source": [
    "# policy_tokenizer_config = policy_tokenizer.config\n",
    "# reward_tokenizer_config = reward_tokenizer.config\n",
    "\n",
    "# print(\"policy_tokenizer_config\", policy_tokenizer_config)\n",
    "# print(\"reward_tokenizer_config\", reward_tokenizer_config)\n",
    "# print(reward_model.config.pad_token_id, old_policy.config.pad_token_id)\n",
    "# assert policy_tokenizer_config == reward_tokenizer.config\n",
    "# print(tokenizer.pad_token_id)\n",
    "dataset_id = \"Dahoas/full-hh-rlhf\"\n",
    "dataset = load_dataset(dataset_id)\n",
    "\n",
    "train_dataset = CustomDataset(policy_tokenizer1, dataset['train'])\n",
    "eval_dataset = CustomDataset(policy_tokenizer1, dataset['test'])\n",
    "\n",
    "# print(train_dataset.column_names)\n",
    "print(len(train_dataset))\n",
    "print(len(eval_dataset))\n",
    "batch_size = 4\n",
    "dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "max_length=256+128 # total sequence length input+output\n",
    "gamma=1.\n",
    "c1=1.\n",
    "c2=1.\n",
    "eps=0.1\n",
    "epochs=1\n",
    "learning_rate = 5e-5\n",
    "weight_decay = 0.01\n",
    "# policy= model.base_model # the underlying LM? dont keep shared parameters, doesnt work, freeze reward model, make copy for policy\n",
    "ppo_iters_per_batch =4 # gotta do 4 updates per batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7dcb0c41-fd63-4494-8364-f9a8658ac674",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset[0].input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "362e4aff-6ccc-4ed6-9849-cea61b75c5a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "  0%|          | 0/1 [00:03<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "cannot reshape tensor of 0 elements into shape [-1, 0] because the unspecified dimension size -1 can be any value and is ambiguous",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[56], line 72\u001b[0m\n\u001b[1;32m     70\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(useful_output_length):\n\u001b[0;32m---> 72\u001b[0m     reversed_rewards\u001b[38;5;241m.\u001b[39mappend(\u001b[43mreward_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcatenated_input\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mt\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m                                         \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mconcatenated_input\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mattention_mask\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mt\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mlogits\u001b[38;5;241m.\u001b[39mcpu())\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28mprint\u001b[39m(reversed_rewards[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     75\u001b[0m     reversed_discounted_rewards\u001b[38;5;241m.\u001b[39mappend(reversed_rewards[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m gamma\u001b[38;5;241m*\u001b[39mreversed_discounted_rewards[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n",
      "File \u001b[0;32m~/conda_envs/rl/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda_envs/rl/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/conda_envs/rl/lib/python3.11/site-packages/transformers/models/gpt2/modeling_gpt2.py:1657\u001b[0m, in \u001b[0;36mGPT2ForSequenceClassification.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1649\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1650\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1651\u001b[0m \u001b[38;5;124;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[1;32m   1652\u001b[0m \u001b[38;5;124;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[1;32m   1653\u001b[0m \u001b[38;5;124;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[1;32m   1654\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1655\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1657\u001b[0m transformer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1658\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1659\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1660\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1661\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1662\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1663\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1664\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1665\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1666\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1667\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1668\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1669\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1670\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m transformer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1671\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscore(hidden_states)\n",
      "File \u001b[0;32m~/conda_envs/rl/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda_envs/rl/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/conda_envs/rl/lib/python3.11/site-packages/transformers/models/gpt2/modeling_gpt2.py:1006\u001b[0m, in \u001b[0;36mGPT2Model.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1004\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwarn_if_padding_and_no_attention_mask(input_ids, attention_mask)\n\u001b[1;32m   1005\u001b[0m     input_shape \u001b[38;5;241m=\u001b[39m input_ids\u001b[38;5;241m.\u001b[39msize()\n\u001b[0;32m-> 1006\u001b[0m     input_ids \u001b[38;5;241m=\u001b[39m \u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_shape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1007\u001b[0m     batch_size \u001b[38;5;241m=\u001b[39m input_ids\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1008\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: cannot reshape tensor of 0 elements into shape [-1, 0] because the unspecified dimension size -1 can be any value and is ambiguous"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "optimizer = optim.AdamW(new_policy.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "for e in tqdm(range(epochs)):\n",
    "    for batch_ in dataloader:\n",
    "        # print(batch.keys())\n",
    "        # print(\"batch shape\", batch[\"input_ids\"].shape)\n",
    "        batch= policy_tokenizer1(batch_, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=256).to(device)\n",
    "        copy_model_parameters(old_policy, new_policy)\n",
    "        # inputs=policy_tokenizer(batch, return_tensors=\"pt\").to(device)\n",
    "        outputs_old_policy=old_policy.generate(input_ids=batch.input_ids, attention_mask=batch.attention_mask, \\\n",
    "                                               max_length=max_length, num_return_sequences=1, return_dict_in_generate=True,\\\n",
    "                                              output_scores=True)\n",
    "        \n",
    "        # outputs_old_policy=old_policy(batch, max_length=max_length)\n",
    "        # print(outputs_old_policy)\n",
    "        # print(outputs_old_policy.sequences.shape)\n",
    "        # print(len(outputs_old_policy.scores))\n",
    "        # print(outputs_old_policy.scores[0].shape)\n",
    "        logits_old_policy = torch.stack(outputs_old_policy.scores, dim=1).to(device) # logits for generated sequence\n",
    "        # print(logits_old_policy.shape)\n",
    "        outputs_ids_old_policy = logits_old_policy.argmax(-1)\n",
    "        # outputs_ids_old_policy = outputs_old_policy.sequences\n",
    "        # logits_old_policy = outputs_old_policy.scores\n",
    "        # print(logits_old_policy.shape)\n",
    "        old_policy_logprobs = logprobs_from_logits(logits_old_policy, outputs_ids_old_policy)\n",
    "        # print(old_policy_logprobs.shape)\n",
    "        # values_old=linear(outputs_old_policy.hidden_state[-1])\n",
    "        episode_length = max_length - batch.input_ids.shape[1]\n",
    "        # print(episode_length)\n",
    "        # freeze_model(old_policy)\n",
    "        \n",
    "        # mini_batch_gen_output_length = episode_length .....\n",
    "        # aint doing mini-batches as its not done in the paper we use as reference (mini-batch of size 1 means takimng whole batch itself as mini-batch)\n",
    "        output_attn_mask=(outputs_ids_old_policy != old_policy.config.pad_token_id).float().to(device)\n",
    "        assert output_attn_mask.shape == old_policy_logprobs.shape, (output_attn_mask.shape, old_policy_prob.shape) \n",
    "        old_policy_output_logprob = masked_mean(old_policy_logprobs, output_attn_mask)\n",
    "        # print(old_policy_output_logprob)\n",
    "        useful_output_length = int(torch.max(torch.sum(output_attn_mask, dim=1,keepdim=True)).cpu().item())\n",
    "        # print(useful_output_length)\n",
    "        \n",
    "        concatenated_input = {\n",
    "                                'input_ids': torch.cat([batch[\"input_ids\"], outputs_ids_old_policy], dim=1).to(device),\n",
    "                                'attention_mask': torch.cat([batch[\"attention_mask\"], output_attn_mask], dim=1).to(device)\n",
    "                            }\n",
    "        for iter in range(ppo_iters_per_batch):\n",
    "            outputs_new_policy=new_policy.generate(input_ids=batch.input_ids, attention_mask=batch.attention_mask, \\\n",
    "                                               max_length=max_length, num_return_sequences=1, return_dict_in_generate=True,\\\n",
    "                                              output_scores=True)\n",
    "            logits_new_policy = torch.stack(outputs_new_policy.scores, dim=1).to(device)\n",
    "            assert logits_new_policy.shape[1] == episode_length, (logits_new_policy.shape[1], episode_length)\n",
    "            # print(outputs_new_policy.keys())\n",
    "            \n",
    "            hidden_state = new_policy.transformer(input_ids=batch.input_ids, attention_mask=batch.attention_mask, return_dict=True).last_hidden_state\n",
    "            # print(new_outs_.keys())\n",
    "            # hidden_state=new_outs_.hidden_states[-1]\n",
    "            # print(hidden_state.shape)\n",
    "            values_new=linear(hidden_state[:,-(episode_length+1):-1,:])\n",
    "            # outputs_ids_old_policy = outputs_old_policy.logits.argmax(-1)\n",
    "            # logits_old_policy = outputs_old_policy.logits\n",
    "\n",
    "            # discounted_rewards=[]\n",
    "            reversed_discounted_rewards=[0.]\n",
    "            # rewards=[]\n",
    "            reversed_rewards=[]\n",
    "            # print(concatenated_input[\"input_ids\"].shape)\n",
    "            optimizer.zero_grad()\n",
    "            for t in range(useful_output_length):\n",
    "                reversed_rewards.append(reward_model(input_ids=concatenated_input[\"input_ids\"][:,:-t],\\\n",
    "                                                     attention_mask= concatenated_input[\"attention_mask\"][:,:-t]).logits.cpu())\n",
    "                print(reversed_rewards[-1].shape)\n",
    "                reversed_discounted_rewards.append(reversed_rewards[-1] + gamma*reversed_discounted_rewards[-1])\n",
    "                # reversed_advantages.append(reversed_rewards[-1] + gamma*reversed_advantages[-1]\n",
    "            reversed_discounted_rewards.pop(0)\n",
    "            reversed_rewards = torch.Tensor(reversed_rewards).to(device)\n",
    "            print(reversed_rewards.shape)\n",
    "            reversed_discounted_rewards=torch.Tensor(reversed_discounted_rewards).to(device)\n",
    "            print(reversed_discounted_rewards.shape)\n",
    "            \n",
    "            rewards=reversed_rewards.flip(0)\n",
    "            discounted_rewards=reversed_discounted_rewards.flip(0)\n",
    "            advantages=-values_new + discounted_rewards\n",
    "            \n",
    "            new_policy_logprobs = logprobs_from_logits(logits_new_policy, outputs_ids_old_policy)\n",
    "            assert output_attn_mask.shape == new_policy_logprobs.shape, (output_attn_mask.shape, new_policy_prob.shape) \n",
    "            new_policy_output_logprob = (masked_mean(new_policy_logprobs, output_attn_mask))\n",
    "            ratio = torch.exp(new_policy_output_logprob - old_policy_output_logprob)\n",
    "            print(\"ratio shape\", ratio.shape)\n",
    "            print(\"advantage shape\", advantage.shape)\n",
    "            loss = torch.mean(torch.mul(advantages,torch.clamp(ratio, 1-eps, 1+eps))) ## clip loss\n",
    "            loss -= c1*torch.mean(advantages**2) ## mse loss \n",
    "            loss += c2* entropy_from_logits(logits_new_policy)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ee7bcd-c920-44ce-9499-100aa8e9b0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # new_policy_output_logprob = (masked_mean(new_policy_logprobs, output_attn_mask))\n",
    "\n",
    "# # ratio = torch.exp(new_policy_output_logprob - old_policy_output_logprob)\n",
    "# # print(old_policy_output_logprob, new_policy_output_logprob)\n",
    "# # print(\"ratio\", ratio)\n",
    "# # print(\"advantage shape\", advantages.shape)\n",
    "# l_clip = torch.mean(torch.mul(advantages,torch.clamp(ratio, 1-eps, 1+eps))) ## clip loss\n",
    "# loss -= c1*torch.mean(advantages**2) ## mse loss \n",
    "# loss += c2* entropy_from_logits(logits_new_policy)\n",
    "\n",
    "# loss.backward()\n",
    "# optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8b3802-cb40-458c-8ec8-123e1a130ff0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "rl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
