{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import GPT2Config, GPT2LMHeadModel, GPT2Tokenizer, AutoModelForSequenceClassification\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparams\n",
    "batch_size=32\n",
    "max_length=256+128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, examples):\n",
    "        self.examples=examples\n",
    "        # self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.examples[idx][\"prompt\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "NVIDIA A100-SXM4-80GB\n"
     ]
    }
   ],
   "source": [
    "# GPU Config\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "print(torch.cuda.get_device_name(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "smadala2/gpt2_ppo\n"
     ]
    }
   ],
   "source": [
    "# Load HF PPO Model\n",
    "model_id = \"smadala2/gpt2_ppo\"\n",
    "model = GPT2LMHeadModel.from_pretrained(model_id)\n",
    "config = GPT2Config.from_pretrained(model_id)\n",
    "print(model.config._name_or_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Reward Model\n",
    "def freeze_model(model):\n",
    "    for name, param in model.named_parameters():\n",
    "        param.requires_grad = False\n",
    "        \n",
    "reward_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "                'Ray2333/gpt2-large-harmless-reward_model',\n",
    "                num_labels=1).to(device)\n",
    "\n",
    "freeze_model(reward_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting pad token\n",
      "Setting pad token id\n",
      "50257\n",
      "<|endoftext|>\n",
      "50256\n"
     ]
    }
   ],
   "source": [
    "# Load tokenizer\n",
    "tokenizer_id = \"gpt2\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(tokenizer_id, padding=\"max_length\", direction=\"left\", padding_side=\"left\", max_length=256, length=256)\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    print(\"Setting pad token\")\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "if model.config.pad_token_id is None:\n",
    "    print(\"Setting pad token id\")\n",
    "    model.config.pad_token_id = model.config.eos_token_id\n",
    "\n",
    "print(len(tokenizer))\n",
    "print(tokenizer.pad_token)\n",
    "print(model.config.pad_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12451\n",
      "\n",
      "\n",
      "Human: I've been seeing a lot of slugs outside recently, even crawling up trees. Should I do something about them, or just let them be?\n",
      "\n",
      "Assistant:\n",
      "390\n"
     ]
    }
   ],
   "source": [
    "# Load test dataset\n",
    "dataset_id = \"Dahoas/full-hh-rlhf\"\n",
    "dataset = load_dataset(dataset_id)\n",
    "test_dataset = CustomDataset(dataset['test'])\n",
    "test_dataloader=DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(len(test_dataset))\n",
    "print(test_dataset[0])\n",
    "print(len(test_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 0\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Cannot handle batch sizes > 1 if no padding token is defined.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 24\u001b[0m\n\u001b[1;32m     18\u001b[0m outputs_attn_mask \u001b[38;5;241m=\u001b[39m (outputs_ids_test \u001b[38;5;241m!=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpad_token_id)\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     20\u001b[0m concatenated_input \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m: torch\u001b[38;5;241m.\u001b[39mcat([inputs_test[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m], outputs_ids_test], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device),\n\u001b[1;32m     21\u001b[0m                       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m: torch\u001b[38;5;241m.\u001b[39mcat([inputs_test[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m], outputs_attn_mask], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     22\u001b[0m                         }\n\u001b[0;32m---> 24\u001b[0m reward \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean(\u001b[43mreward_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcatenated_input\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mconcatenated_input\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mattention_mask\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mlogits\u001b[38;5;241m.\u001b[39mcpu())\u001b[38;5;241m.\u001b[39mitem() \n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Collect rewards and losses \u001b[39;00m\n\u001b[1;32m     27\u001b[0m rewards \u001b[38;5;241m=\u001b[39m rewards \u001b[38;5;241m+\u001b[39m [reward]\u001b[38;5;241m*\u001b[39mbatch_size\n",
      "File \u001b[0;32m~/miniconda3/envs/cs443/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/cs443/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py:1679\u001b[0m, in \u001b[0;36mGPT2ForSequenceClassification.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1675\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1676\u001b[0m     batch_size, sequence_length \u001b[38;5;241m=\u001b[39m inputs_embeds\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m   1678\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[0;32m-> 1679\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpad_token_id \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m batch_size \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1680\u001b[0m ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot handle batch sizes > 1 if no padding token is defined.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1681\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpad_token_id \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1682\u001b[0m     sequence_lengths \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mAssertionError\u001b[0m: Cannot handle batch sizes > 1 if no padding token is defined."
     ]
    }
   ],
   "source": [
    "# Perform evaluation on dataset w/ PPO GPT2\n",
    "model.eval()\n",
    "model.to(\"cuda\")\n",
    "\n",
    "rewards = []\n",
    "test_losses = []\n",
    "\n",
    "for i, batch in enumerate(test_dataloader):\n",
    "    print(f\"Batch: {i}\")\n",
    "    inputs_test = tokenizer(batch, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=256).to(device)\n",
    "    \n",
    "    outputs_test = model.generate(input_ids=inputs_test.input_ids, attention_mask=inputs_test.attention_mask, \\\n",
    "        max_length=max_length, num_return_sequences=1, return_dict_in_generate=True, \\\n",
    "            output_scores=True)\n",
    "    \n",
    "    logits_test = torch.stack(outputs_test.scores, dim=1).to(device)\n",
    "    outputs_ids_test = logits_test.argmax(-1) \n",
    "    outputs_attn_mask = (outputs_ids_test != model.config.pad_token_id).float().to(device)\n",
    "\n",
    "    concatenated_input = {'input_ids': torch.cat([inputs_test[\"input_ids\"], outputs_ids_test], dim=1).to(device),\n",
    "                          'attention_mask': torch.cat([inputs_test[\"attention_mask\"], outputs_attn_mask], dim=1).to(device)\n",
    "                            }\n",
    "    \n",
    "    reward = torch.mean(reward_model(input_ids=concatenated_input[\"input_ids\"], attention_mask= concatenated_input[\"attention_mask\"]).logits.cpu()).item() \n",
    "    \n",
    "    # Collect rewards and losses \n",
    "    rewards = rewards + [reward]*batch_size\n",
    "    test_losses.append(1-reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_losses)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs443",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
