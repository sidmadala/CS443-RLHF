{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import GPT2Config, GPT2LMHeadModel, GPT2Tokenizer, AutoModelForSequenceClassification\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparams\n",
    "batch_size=4\n",
    "max_length=256+128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, examples):\n",
    "        self.examples=examples\n",
    "        # self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.examples[idx][\"prompt\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "NVIDIA A100-SXM4-80GB\n"
     ]
    }
   ],
   "source": [
    "# GPU Config\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "print(torch.cuda.get_device_name(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sharma96/conda_envs/rl/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "smadala2/gpt2_ppo\n",
      "gpt2\n"
     ]
    }
   ],
   "source": [
    "# Load HF PPO Model\n",
    "ppo_model_id = \"smadala2/gpt2_ppo\"\n",
    "ppo_model = GPT2LMHeadModel.from_pretrained(ppo_model_id)\n",
    "ppo_config = GPT2Config.from_pretrained(ppo_model_id)\n",
    "print(ppo_model.config._name_or_path)\n",
    "\n",
    "# Load another ppo trained model from HF\n",
    "ppo_model_id2 = \"jtatman/gpt2-open-instruct-v1-Anthropic-hh-rlhf\"\n",
    "ppo_model2 = GPT2LMHeadModel.from_pretrained(ppo_model_id2)\n",
    "\n",
    "# Load HF raw GPT2 Model\n",
    "raw_model_id = \"gpt2\"\n",
    "raw_model = GPT2LMHeadModel.from_pretrained(raw_model_id)\n",
    "raw_config = GPT2Config.from_pretrained(raw_model_id)\n",
    "print(raw_model.config._name_or_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params1 = list(ppo_model.parameters())\n",
    "# params2 = list(raw_model.parameters())\n",
    "\n",
    "# # Calculate differences\n",
    "# differences = []\n",
    "# for param1, param2 in zip(params1, params2):\n",
    "#     diff = torch.norm(param1 - param2, p=2)  # L2 norm\n",
    "#     differences.append(diff.item())\n",
    "\n",
    "# # Aggregate differences\n",
    "# total_difference = sum(differences)\n",
    "# print(total_difference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Reward Model\n",
    "def freeze_model(model):\n",
    "    for name, param in model.named_parameters():\n",
    "        param.requires_grad = False\n",
    "        \n",
    "reward_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "                'Ray2333/gpt2-large-harmless-reward_model',\n",
    "                num_labels=1).to(device)\n",
    "\n",
    "freeze_model(reward_model)\n",
    "freeze_model(ppo_model)\n",
    "freeze_model(ppo_model2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting pad token id\n",
      "Setting pad token id\n",
      "50257\n",
      "<|endoftext|>\n",
      "50256\n",
      "50256\n",
      "50256\n"
     ]
    }
   ],
   "source": [
    "# Load tokenizer\n",
    "tokenizer_id = \"gpt2\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(tokenizer_id, padding=\"max_length\", direction=\"left\", padding_side=\"left\", max_length=256, length=256)\n",
    "\n",
    "# if tokenizer.pad_token is None:\n",
    "#     print(\"Setting pad token\")\n",
    "#     tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "# if model.config.pad_token_id is None:\n",
    "#     print(\"Setting pad token id\")\n",
    "#     model.config.pad_token_id = model.config.eos_token_id\n",
    "\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    print(\"Setting pad token id\")\n",
    "    ppo_model.config.pad_token_id = ppo_model.config.eos_token_id\n",
    "    ppo_model2.config.pad_token_id = ppo_model2.config.eos_token_id\n",
    "    raw_model.config.pad_token_id = raw_model.config.eos_token_id\n",
    "    \n",
    "if reward_model.config.pad_token_id is None:\n",
    "    reward_model.config.pad_token_id = ppo_model.config.pad_token_id\n",
    "    print(\"Setting pad token id\")\n",
    "\n",
    "print(len(tokenizer))\n",
    "print(tokenizer.pad_token)\n",
    "print(ppo_model.config.pad_token_id)\n",
    "print(raw_model.config.pad_token_id)\n",
    "print(reward_model.config.pad_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12451\n",
      "\n",
      "\n",
      "Human: I've been seeing a lot of slugs outside recently, even crawling up trees. Should I do something about them, or just let them be?\n",
      "\n",
      "Assistant:\n",
      "3113\n"
     ]
    }
   ],
   "source": [
    "# Load test dataset\n",
    "dataset_id = \"Dahoas/full-hh-rlhf\"\n",
    "dataset = load_dataset(dataset_id)\n",
    "test_dataset = CustomDataset(dataset['test'])\n",
    "\n",
    "print(len(test_dataset))\n",
    "print(test_dataset[0])\n",
    "\n",
    "DEBUG = False\n",
    "if DEBUG == True:\n",
    "    # Get 10 samples for testing\n",
    "    test_dataset = test_dataset[:10]    \n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "else:\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "print(len(test_dataloader))\n",
    "# # Print some samples\n",
    "# for i, sample in enumerate(test_dataloader):\n",
    "#     print(i, sample)\n",
    "#     if i == 10:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(model, inputs, device):\n",
    "    outputs_test = model.generate(input_ids=inputs.input_ids, attention_mask=inputs.attention_mask, \\\n",
    "        max_length=max_length, num_return_sequences=1, return_dict_in_generate=True, \\\n",
    "            output_scores=True)\n",
    "    \n",
    "    logits_test = torch.stack(outputs_test.scores, dim=1).to(device)\n",
    "    outputs_ids_test = logits_test.argmax(-1) \n",
    "    outputs_attn_mask = (outputs_ids_test != ppo_model.config.pad_token_id).float().to(device)\n",
    "\n",
    "    concatenated_input = {'input_ids': torch.cat([inputs[\"input_ids\"], outputs_ids_test], dim=1).to(device),\n",
    "                          'attention_mask': torch.cat([inputs[\"attention_mask\"], outputs_attn_mask], dim=1).to(device)\n",
    "                            }\n",
    "    \n",
    "    reward = torch.mean(reward_model(input_ids=concatenated_input[\"input_ids\"], attention_mask= concatenated_input[\"attention_mask\"]).logits.cpu()).item() \n",
    "    return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "1it [00:12, 12.93s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "2it [00:26, 13.10s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "3it [00:39, 13.16s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "4it [00:49, 11.79s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "5it [00:59, 11.30s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "6it [01:06,  9.81s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "7it [01:25, 12.71s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "8it [01:42, 14.30s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "9it [01:58, 14.79s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "10it [02:08, 13.12s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "11it [02:14, 11.08s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "12it [02:29, 12.37s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "13it [02:45, 13.45s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "14it [03:01, 14.03s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "# Perform evaluation on dataset w/ PPO GPT2\n",
    "raw_model.eval()\n",
    "ppo_model.eval()\n",
    "ppo_model2.eval()\n",
    "\n",
    "raw_model.to(device)\n",
    "ppo_model.to(device)\n",
    "ppo_model2.to(device)\n",
    "\n",
    "raw_rewards, raw_test_losses = [], []\n",
    "ppo_rewards, ppo_test_losses = [], []\n",
    "ppo_rewards2, ppo_test_losses2 = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    \n",
    "    for i, batch in tqdm(enumerate(test_dataloader)):\n",
    "        inputs_test = tokenizer(batch, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=256).to(device)\n",
    "        \n",
    "        # Forward pass for GPT2-PPO\n",
    "        ppo_reward = forward_pass(ppo_model, inputs_test, device)\n",
    "        ppo_reward2 = forward_pass(ppo_model2, inputs_test, device)\n",
    "        \n",
    "        # Forward pass for GPT2-Raw\n",
    "        raw_reward = forward_pass(raw_model, inputs_test, device)\n",
    "    \n",
    "        # Collect rewards and losses \n",
    "        ppo_rewards = ppo_rewards + [ppo_reward]*batch_size\n",
    "        ppo_test_losses.append(1-ppo_reward)\n",
    "\n",
    "        ppo_rewards2 = ppo_rewards2 + [ppo_reward2]*batch_size\n",
    "        ppo_test_losses2.append(1-ppo_reward2)\n",
    "        \n",
    "        raw_rewards = raw_rewards + [raw_reward]*batch_size\n",
    "        raw_test_losses.append(1-raw_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print and save PPO results\n",
    "print(\"PPO Test Loss: \", sum(ppo_test_losses)/len(ppo_test_losses))\n",
    "print(\"PPO Test Reward: \", sum(ppo_rewards)/len(ppo_rewards))\n",
    "\n",
    "# Print and save Raw results\n",
    "print(\"Raw Test Loss: \", sum(raw_test_losses)/len(raw_test_losses))\n",
    "print(\"Raw Test Reward: \", sum(raw_rewards)/len(raw_rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(ppo_rewards))\n",
    "print(ppo_rewards[0:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "rl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
