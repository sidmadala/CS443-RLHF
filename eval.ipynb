{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import GPT2Config, GPT2LMHeadModel, GPT2Tokenizer, AutoModelForSequenceClassification\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparams\n",
    "batch_size=128\n",
    "max_length=256+128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, examples):\n",
    "        self.examples=examples\n",
    "        # self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.examples[idx][\"prompt\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "NVIDIA A100-SXM4-80GB\n"
     ]
    }
   ],
   "source": [
    "# GPU Config\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "print(torch.cuda.get_device_name(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "smadala2/gpt2_ppo\n",
      "gpt2\n"
     ]
    }
   ],
   "source": [
    "# Load HF PPO Model\n",
    "ppo_model_id = \"smadala2/gpt2_ppo\"\n",
    "ppo_model = GPT2LMHeadModel.from_pretrained(ppo_model_id)\n",
    "ppo_config = GPT2Config.from_pretrained(ppo_model_id)\n",
    "print(ppo_model.config._name_or_path)\n",
    "\n",
    "# Load HF raw GPT2 Model\n",
    "raw_model_id = \"gpt2\"\n",
    "raw_model = GPT2LMHeadModel.from_pretrained(raw_model_id)\n",
    "raw_config = GPT2Config.from_pretrained(raw_model_id)\n",
    "print(raw_model.config._name_or_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Reward Model\n",
    "def freeze_model(model):\n",
    "    for name, param in model.named_parameters():\n",
    "        param.requires_grad = False\n",
    "        \n",
    "reward_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "                'Ray2333/gpt2-large-harmless-reward_model',\n",
    "                num_labels=1).to(device)\n",
    "\n",
    "freeze_model(reward_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting pad token id\n",
      "Setting pad token id\n",
      "50257\n",
      "<|endoftext|>\n",
      "50256\n",
      "50256\n",
      "50256\n"
     ]
    }
   ],
   "source": [
    "# Load tokenizer\n",
    "tokenizer_id = \"gpt2\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(tokenizer_id, padding=\"max_length\", direction=\"left\", padding_side=\"left\", max_length=256, length=256)\n",
    "\n",
    "# if tokenizer.pad_token is None:\n",
    "#     print(\"Setting pad token\")\n",
    "#     tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "# if model.config.pad_token_id is None:\n",
    "#     print(\"Setting pad token id\")\n",
    "#     model.config.pad_token_id = model.config.eos_token_id\n",
    "\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    print(\"Setting pad token id\")\n",
    "    ppo_model.config.pad_token_id = ppo_model.config.eos_token_id\n",
    "    raw_model.config.pad_token_id = raw_model.config.eos_token_id\n",
    "    \n",
    "if reward_model.config.pad_token_id is None:\n",
    "    reward_model.config.pad_token_id = ppo_model.config.pad_token_id\n",
    "    print(\"Setting pad token id\")\n",
    "\n",
    "print(len(tokenizer))\n",
    "print(tokenizer.pad_token)\n",
    "print(ppo_model.config.pad_token_id)\n",
    "print(raw_model.config.pad_token_id)\n",
    "print(reward_model.config.pad_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12451\n",
      "\n",
      "\n",
      "Human: I've been seeing a lot of slugs outside recently, even crawling up trees. Should I do something about them, or just let them be?\n",
      "\n",
      "Assistant:\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "# Load test dataset\n",
    "dataset_id = \"Dahoas/full-hh-rlhf\"\n",
    "dataset = load_dataset(dataset_id)\n",
    "test_dataset = CustomDataset(dataset['test'])\n",
    "\n",
    "print(len(test_dataset))\n",
    "print(test_dataset[0])\n",
    "\n",
    "DEBUG = True\n",
    "if DEBUG == True:\n",
    "    # Get 10 samples for testing\n",
    "    test_dataset = test_dataset[:10]    \n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "else:\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "print(len(test_dataloader))\n",
    "# # Print some samples\n",
    "# for i, sample in enumerate(test_dataloader):\n",
    "#     print(i, sample)\n",
    "#     if i == 10:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(model, inputs, device):\n",
    "    outputs_test = ppo_model.generate(input_ids=inputs.input_ids, attention_mask=inputs.attention_mask, \\\n",
    "        max_length=max_length, num_return_sequences=1, return_dict_in_generate=True, \\\n",
    "            output_scores=True)\n",
    "    \n",
    "    logits_test = torch.stack(outputs_test.scores, dim=1).to(device)\n",
    "    outputs_ids_test = logits_test.argmax(-1) \n",
    "    outputs_attn_mask = (outputs_ids_test != ppo_model.config.pad_token_id).float().to(device)\n",
    "\n",
    "    concatenated_input = {'input_ids': torch.cat([inputs[\"input_ids\"], outputs_ids_test], dim=1).to(device),\n",
    "                          'attention_mask': torch.cat([inputs[\"attention_mask\"], outputs_attn_mask], dim=1).to(device)\n",
    "                            }\n",
    "    \n",
    "    reward = torch.mean(reward_model(input_ids=concatenated_input[\"input_ids\"], attention_mask= concatenated_input[\"attention_mask\"]).logits.cpu()).item() \n",
    "    return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "1it [00:01,  1.87s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "2it [00:03,  1.86s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "3it [00:05,  1.86s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "4it [00:07,  1.86s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "5it [00:09,  1.86s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "6it [00:11,  1.86s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "7it [00:13,  1.87s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "8it [00:14,  1.87s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "9it [00:16,  1.87s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "10it [00:18,  1.87s/it]\n"
     ]
    }
   ],
   "source": [
    "# Perform evaluation on dataset w/ PPO GPT2\n",
    "raw_model.eval()\n",
    "ppo_model.eval()\n",
    "\n",
    "raw_model.to(device)\n",
    "ppo_model.to(device)\n",
    "\n",
    "raw_rewards, raw_test_losses = [], []\n",
    "ppo_rewards, ppo_test_losses = [], []\n",
    "\n",
    "for i, batch in tqdm(enumerate(test_dataloader)):\n",
    "    inputs_test = tokenizer(batch, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=256).to(device)\n",
    "    \n",
    "    # Forward pass for GPT2-PPO\n",
    "    ppo_reward = forward_pass(ppo_model, inputs_test, device) \n",
    "    \n",
    "    # Forward pass for GPT2-Raw\n",
    "    raw_reward = forward_pass(raw_model, inputs_test, device)\n",
    "\n",
    "    # Collect rewards and losses \n",
    "    ppo_rewards = ppo_rewards + [ppo_reward]*batch_size\n",
    "    ppo_test_losses.append(1-ppo_reward)\n",
    "    \n",
    "    raw_rewards = raw_rewards + [raw_reward]*batch_size\n",
    "    raw_test_losses.append(1-raw_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PPO Test Loss:  -0.779716445505619\n",
      "PPO Test Reward:  1.7797164455056191\n",
      "Raw Test Loss:  -0.779716445505619\n",
      "Raw Test Reward:  1.7797164455056191\n"
     ]
    }
   ],
   "source": [
    "# Print and save PPO results\n",
    "print(\"PPO Test Loss: \", sum(ppo_test_losses)/len(ppo_test_losses))\n",
    "print(\"PPO Test Reward: \", sum(ppo_rewards)/len(ppo_rewards))\n",
    "\n",
    "# Print and save Raw results\n",
    "print(\"Raw Test Loss: \", sum(raw_test_losses)/len(raw_test_losses))\n",
    "print(\"Raw Test Reward: \", sum(raw_rewards)/len(raw_rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1280\n",
      "[2.6294021606445312, 2.6294021606445312, 2.6294021606445312, 2.6294021606445312, 2.6294021606445312, 2.6294021606445312, 2.6294021606445312, 2.6294021606445312, 2.6294021606445312, 2.6294021606445312, 2.6294021606445312, 2.6294021606445312, 2.6294021606445312, 2.6294021606445312, 2.6294021606445312, 2.6294021606445312, 2.6294021606445312, 2.6294021606445312, 2.6294021606445312, 2.6294021606445312, 2.6294021606445312, 2.6294021606445312, 2.6294021606445312, 2.6294021606445312, 2.6294021606445312, 2.6294021606445312, 2.6294021606445312, 2.6294021606445312, 2.6294021606445312, 2.6294021606445312, 2.6294021606445312, 2.6294021606445312, 2.6294021606445312, 2.6294021606445312, 2.6294021606445312, 2.6294021606445312, 2.6294021606445312, 2.6294021606445312, 2.6294021606445312, 2.6294021606445312, 2.6294021606445312, 2.6294021606445312, 2.6294021606445312, 2.6294021606445312, 2.6294021606445312, 2.6294021606445312, 2.6294021606445312, 2.6294021606445312, 2.6294021606445312, 2.6294021606445312]\n"
     ]
    }
   ],
   "source": [
    "print(len(ppo_rewards))\n",
    "print(ppo_rewards[0:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs443",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
